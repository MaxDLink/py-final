{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31f0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "!pip install faiss-cpu sentence-transformers langchain langchain_community langchain-huggingface langchain-ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e15971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 non-null articles for the vector store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197702/1476988221.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load your CSV dataset\n",
    "df = pd.read_csv('/mnt/f/datasets/data.csv')\n",
    "\n",
    "# Filter for valid full content\n",
    "texts = df['full_content'].dropna().sample(n=100, random_state=42).tolist()\n",
    "\n",
    "# Show how many articles we kept\n",
    "print(f\"Using {len(texts)} non-null articles for the vector store.\")\n",
    "\n",
    "# Initialize embeddings and build FAISS index\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51759ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = OllamaLLM(model=\"mistral\")\n",
    "\n",
    "krag_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"triples\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Knowledge Graph Triples:\n",
    "{triples}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "krag_chain = krag_template | llm | StrOutputParser()\n",
    "\n",
    "rag_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = rag_template | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1eeec19-7522-474d-bce9-107559e86704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extract subject-predicate-object triples from each sentence\n",
    "def extract_relationships(doc):\n",
    "    relationships = []\n",
    "    for sent in doc.sents:\n",
    "        root = sent.root\n",
    "        subj = next((child for child in root.children if child.dep_ == \"nsubj\"), None)\n",
    "        obj = next((child for child in root.children if child.dep_ in [\"dobj\", \"pobj\"]), None)\n",
    "        if subj and obj:\n",
    "            relationships.append((subj.text, root.lemma_, obj.text))\n",
    "    return relationships\n",
    "\n",
    "# Build KG with NetworkX\n",
    "G = nx.DiGraph()\n",
    "all_triples = []\n",
    "\n",
    "for text in texts:  # 'texts' is your list of sampled full_content entries\n",
    "    doc = nlp(text)\n",
    "    triples = extract_relationships(doc)\n",
    "    all_triples.extend(triples)\n",
    "    for subj, pred, obj in triples:\n",
    "        G.add_edge(subj, obj, relation=pred)\n",
    "\n",
    "# Prepare for embedding\n",
    "triple_texts = [f\"{s} —[{p}]→ {o}\" for s, p, o in G.edges(data=\"relation\")]\n",
    "triple_tuples = [(s, p, o) for s, o, p in G.edges(data=\"relation\")]\n",
    "\n",
    "# Embed triples using sentence-transformers\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "triple_embeddings = embedder.encode(triple_texts)\n",
    "\n",
    "# Build FAISS index for triple search\n",
    "index = faiss.IndexFlatL2(triple_embeddings.shape[1])\n",
    "index.add(np.array(triple_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a992d8d-5518-41fb-b3ea-f495b75cb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show KG \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# pos = nx.spring_layout(G, k=0.5)\n",
    "# nx.draw(G, pos, with_labels=True, node_size=500, font_size=8)\n",
    "# edge_labels = nx.get_edge_attributes(G, 'relation')\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
    "# plt.title(\"KG\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a776b26-8678-410b-93d8-6ab106284492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to top N nodes by degree (most connected)\n",
    "N = 50\n",
    "degree_dict = dict(G.degree())\n",
    "top_nodes = sorted(degree_dict, key=degree_dict.get, reverse=True)[:N]\n",
    "subgraph = G.subgraph(top_nodes)\n",
    "\n",
    "# plt.figure(figsize=(12, 9))\n",
    "# pos = nx.spring_layout(subgraph, k=0.7)\n",
    "# nx.draw(subgraph, pos, with_labels=True, node_size=800, font_size=9, node_color=\"#69b3a2\")\n",
    "# edge_labels = nx.get_edge_attributes(subgraph, 'relation')\n",
    "# nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels, font_size=7)\n",
    "# plt.title(\"Top 50 Entity Knowledge Graph\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "876fe65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format triples for LLM prompt\n",
    "def format_triples(triples):\n",
    "    return [f\"{s} —[{p}]→ {o}\" for s, p, o in triples]\n",
    "\n",
    "# Score recall based on how many ground truth triples are reflected in the answer for KRAG \n",
    "def score_recall_KRAG(triples, answer):\n",
    "    total_parts = 0\n",
    "    matched_parts = 0\n",
    "    for s, p, o in triples:\n",
    "        for part in [s, p, o]: # GT Subject, predicate, and object \n",
    "            total_parts += 1\n",
    "            if part.lower() in answer.lower(): # if the subject, predicate, and object ground truth appear in LLM answer, then increment recall score\n",
    "                # print(\"triple used: \", part.lower())\n",
    "                matched_parts += 1\n",
    "    print(\"Triple Count:\", matched_parts) # threshold \n",
    "    return matched_parts / total_parts if total_parts else 0\n",
    "\n",
    "# Score recall based on how well ground truth context is reflected in the answer for RAG \n",
    "def score_recall_RAG(context, answer): \n",
    "    print(\"number of words in context also in answer: \", sum(1 for word in answer.split() if word.lower() in context.lower())) # threshold \n",
    "    return sum(1 for word in answer.split() if word.lower() in context.lower()) / len(answer.split())\n",
    "    \n",
    "# Retrieve documents and generate an answer with context\n",
    "def rag_query(question, k = 2):\n",
    "    docs = vectorstore.similarity_search(question, k=k)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    answer = rag_chain.invoke({\"context\": context, \"question\": question}).strip()\n",
    "    return answer, context\n",
    "\n",
    "\n",
    "def krag_query(question, k=5):\n",
    "    # Embed the question and retrieve top-k triples from FAISS\n",
    "    q_embed = embedder.encode([question])\n",
    "    _, I = index.search(np.array(q_embed), k)\n",
    "    \n",
    "    # Get the matching triples\n",
    "    retrieved_triples = [triple_tuples[i] for i in I[0]]\n",
    "    triples_text = \"\\n\".join(format_triples(retrieved_triples))\n",
    "\n",
    "    # Retrieve vectorstore context as usual\n",
    "    context = \"\\n\".join([doc.page_content for doc in vectorstore.similarity_search(question, k=2)])\n",
    "\n",
    "    # Run the LLM with context and embedding-retrieved triples\n",
    "    # print(\"Ground Truth Triples:\", retrieved_triples)\n",
    "    answer = krag_chain.invoke({\"context\": context, \"triples\": triples_text, \"question\": question}).strip()\n",
    "\n",
    "    return answer, retrieved_triples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50fce4ad-a1f0-4c78-b846-14534164d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- What does Ltd. do?\n",
      "- What does stake do?\n",
      "- What does fund do?\n",
      "- What does shares do?\n",
      "- What does % do?\n",
      "- What does MD do?\n",
      "- What does position do?\n",
      "- What does LLC do?\n",
      "- What does Bank do?\n",
      "- What does Management do?\n"
     ]
    }
   ],
   "source": [
    "# Generate sample questions based on entities in KG \n",
    "sample_entities = list(G.nodes())[:10]\n",
    "\n",
    "questions = [f\"What does {entity} do?\" for entity in sample_entities]\n",
    "\n",
    "# Optionally preview them\n",
    "for q in questions:\n",
    "    print(\"-\", q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e130dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What does Ltd. do?\n",
      "Triple Count: 5\n",
      "number of words in context also in answer:  25\n",
      "  KRAG Recall from triples: 0.33\n",
      "\n",
      "  RAG Recall from context: 0.78\n",
      "\n",
      "Question: What does stake do?\n",
      "Triple Count: 9\n",
      "number of words in context also in answer:  62\n",
      "  KRAG Recall from triples: 0.60\n",
      "\n",
      "  RAG Recall from context: 0.65\n",
      "\n",
      "Question: What does fund do?\n",
      "Triple Count: 8\n",
      "number of words in context also in answer:  48\n",
      "  KRAG Recall from triples: 0.53\n",
      "\n",
      "  RAG Recall from context: 0.65\n",
      "\n",
      "Question: What does shares do?\n",
      "Triple Count: 11\n",
      "number of words in context also in answer:  54\n",
      "  KRAG Recall from triples: 0.73\n",
      "\n",
      "  RAG Recall from context: 0.65\n",
      "\n",
      "Question: What does % do?\n",
      "Triple Count: 15\n",
      "number of words in context also in answer:  74\n",
      "  KRAG Recall from triples: 1.00\n",
      "\n",
      "  RAG Recall from context: 0.80\n",
      "\n",
      "Question: What does MD do?\n",
      "Triple Count: 5\n",
      "number of words in context also in answer:  33\n",
      "  KRAG Recall from triples: 0.33\n",
      "\n",
      "  RAG Recall from context: 0.47\n",
      "\n",
      "Question: What does position do?\n",
      "Triple Count: 9\n",
      "number of words in context also in answer:  48\n",
      "  KRAG Recall from triples: 0.60\n",
      "\n",
      "  RAG Recall from context: 0.81\n",
      "\n",
      "Question: What does LLC do?\n",
      "Triple Count: 13\n",
      "number of words in context also in answer:  59\n",
      "  KRAG Recall from triples: 0.87\n",
      "\n",
      "  RAG Recall from context: 0.63\n",
      "\n",
      "Question: What does Bank do?\n",
      "Triple Count: 15\n",
      "number of words in context also in answer:  34\n",
      "  KRAG Recall from triples: 1.00\n",
      "\n",
      "  RAG Recall from context: 0.49\n",
      "\n",
      "Question: What does Management do?\n",
      "Triple Count: 10\n",
      "number of words in context also in answer:  78\n",
      "  KRAG Recall from triples: 0.67\n",
      "\n",
      "  RAG Recall from context: 0.69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    print(f\"Question: {q}\")\n",
    "\n",
    "    rag_answer, rag_context = rag_query(q)\n",
    "    krag_answer, ground_truth = krag_query(q)\n",
    "\n",
    "    krag_recall = score_recall_KRAG(ground_truth, krag_answer)\n",
    "    rag_recall = score_recall_RAG(rag_context, rag_answer)\n",
    "\n",
    "    # print(f\"  RAG Answer:  {rag_answer}\")\n",
    "    # print(f\"  kRAG Answer: {krag_answer}\")\n",
    "    print(f\"  KRAG Recall from triples: {krag_recall:.2f}\\n\")\n",
    "    print(f\"  RAG Recall from context: {rag_recall:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927cd60a-05ab-4001-8308-0d272b58b537",
   "metadata": {},
   "source": [
    "### So now a good thing to graph would be how the triples found in ground truth increases the KRAG recall and how the number of words in context that are also in answer increase the RAG recall to see the influence of both \n",
    "\n",
    "### We can see here that 5 triples found in ground truth does not increase KRAG recall above RAG recall by a significant margin, but when we find 9 triples in ground truth we near RAG recall with 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975047d-6e92-4d31-a8d9-40179b65b584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
